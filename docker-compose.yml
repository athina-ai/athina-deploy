version: "3.9"

services:
  prompt_db:
    profiles: ["core"]
    image: postgres:15.3-alpine
    restart: unless-stopped
    environment:
      - POSTGRES_PASSWORD=${PROMPT_DB_POSTGRES_PASSWORD}
      - POSTGRES_USER=${PROMPT_DB_POSTGRES_USER}
      - POSTGRES_DB=${PROMPT_DB_POSTGRES_DB_NAME}
    volumes:
      - prompt_db_postgres_data:/var/lib/postgresql/data
    container_name: prompt_db
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5

  api:
    profiles: ["core"]
    image: 867387325299.dkr.ecr.eu-central-1.amazonaws.com/athina-api:v1.8.26
    build:
      context: ../athina-api/.
      dockerfile: Dockerfile
    container_name: api
    ports:
      - "9000:9000"
    # Note: This depends_on is only needed if you are using the docker compose db.
    # If you are providing your own database url in the .env file, you can comment the depends_on part.
    depends_on:
      prompt_db:
        condition: service_healthy
    environment:
      - DATABASE_URL=${PROMPT_DATABASE_URL}
      - READ_DATABASE_URL=${PROMPT_DATABASE_URL}
      - ANALYTICS_ROOT_URL=${ANALYTICS_ROOT_URL}
      - AWS_LAMBDA_API_KEY=${AWS_LAMBDA_API_KEY}
      - LAMBDA_TRIGGER_WORKER_FUNCTION_URL=${LAMBDA_TRIGGER_WORKER_FUNCTION_URL}
      - ENV=local
      - NEW_RELIC_ENABLED=false
      - NODE_ENV=development
      - AWS_CLIENT_ID=${AWS_CLIENT_ID}
      - AWS_CLIENT_SECRET=${AWS_CLIENT_SECRET}
      - AWS_REGION=${AWS_REGION}
      - PORT=9000
      - GOOGLE_CLIENT_ID=${GOOGLE_CLIENT_ID}
      - GOOGLE_CLIENT_SECRET=${GOOGLE_CLIENT_SECRET}
      - COOKIE_KEY=${COOKIE_KEY}
      - ROOT_URL=${API_SERVER_ROOT_URL}
      - FRONTEND_URL=${FRONTEND_URL}
      - SERVER_API_SECRET=${SERVER_API_SECRET}
      - ANALYTICS_SERVER_API_SECRET=${ANALYTICS_SERVER_API_SECRET}
      - AIRTABLE_AUTH_TOKEN=${AIRTABLE_AUTH_TOKEN}
      - AIRTABLE_CRM_BASE_ID=${AIRTABLE_CRM_BASE_ID}
      - AIRTABLE_LEADS_TABLE_ID=${AIRTABLE_LEADS_TABLE_ID}
      - NEW_RELIC_TRACER_ENABLED=${NEW_RELIC_TRACER_ENABLED}
      - NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=${NEW_RELIC_DISTRIBUTED_TRACING_ENABLED}
      - NEW_RELIC_SPAN_EVENTS_ENABLED=${NEW_RELIC_SPAN_EVENTS_ENABLED}
      - NEW_RELIC_DATASTORE_INSTANCE_REPORTING_ENABLED=${NEW_RELIC_DATASTORE_INSTANCE_REPORTING_ENABLED}
      - NEW_RELIC_APPLICATION_LOGGING_METRICS_ENABLED=${NEW_RELIC_APPLICATION_LOGGING_METRICS_ENABLED}
      - NEW_RELIC_CODE_LEVEL_METRICS_ENABLED=${NEW_RELIC_CODE_LEVEL_METRICS_ENABLED}
      - NEW_RELIC_APP_NAME=${NEW_RELIC_APP_NAME}
      - NEW_RELIC_LICENSE_KEY=${NEW_RELIC_LICENSE_KEY}
      - NEW_RELIC_LOG_LEVEL=${NEW_RELIC_LOG_LEVEL}
      - POSTHOG_API_KEY=${POSTHOG_API_KEY}
      - PARTNER_API_SECRET=${PARTNER_API_SECRET}
      - JWT_SECRET=${JWT_SECRET}
      - JWT_EXPIRES_IN=${JWT_EXPIRES_IN}
      - LOOPS_NEW_USER_INVITATION_EMAIL_TEMPLATE_ID=${LOOPS_NEW_USER_INVITATION_EMAIL_TEMPLATE_ID}
      - LOOPS_OTP_TRANSACTIONAL_EMAIL_TEMPLATE_ID=${LOOPS_OTP_TRANSACTIONAL_EMAIL_TEMPLATE_ID}
      - LOOPS_API_KEY=${LOOPS_API_KEY}
      - DEMO_ORG_ID=${DEMO_ORG_ID}
      - LOOPS_DATA_EXPORT_DOWNLOAD_LINK_TEMPLATE_ID=${LOOPS_DATA_EXPORT_DOWNLOAD_LINK_TEMPLATE_ID}
      - S3_DATASET_UPLOAD_BUCKET_NAME=${S3_DATASET_UPLOAD_BUCKET_NAME}
      - GATEWAY_URL=${GATEWAY_URL}
      - GATEWAY_SECRET=${GATEWAY_API_SECRET}
      - DEPLOYMENT_TYPE=self-hosted
      - ALLOWED_ORIGINS=${ALLOWED_ORIGINS}
      - ORG_EXCLUDED_FROM_STATS=${ORG_EXCLUDED_FROM_STATS}
    healthcheck:
      test: wget --spider -q http://localhost:9000/api/v1 || exit 1
      start_period: 10s
      interval: 15s
      timeout: 5s
      retries: 5

  jobs_db:
    profiles: ["core"]
    image: postgres:15.3-alpine
    restart: unless-stopped
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_PASSWORD=${JOBS_DB_POSTGRES_PASSWORD}
      - POSTGRES_USER=${JOBS_DB_POSTGRES_USER}
      - POSTGRES_DB=${JOBS_DB_POSTGRES_DB_NAME}
    volumes:
      - jobs_db_postgres_data:/var/lib/postgresql/data
    container_name: jobs_db
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5

  scheduler:
    profiles: ["core"]
    image: 867387325299.dkr.ecr.eu-central-1.amazonaws.com/athina-scheduler:v1.8.2
    build:
      context: ../athina-scheduler/.
      dockerfile: Dockerfile
    container_name: scheduler
    # Note: This depends_on is only needed if you are using the docker compose db.
    # If you are providing your own database url in the .env file, you can comment the depends_on part.
    depends_on:
      api:
        condition: service_healthy
      jobs_db:
        condition: service_healthy
    links:
      - api
    restart: unless-stopped
    environment:
      - DATABASE_URL=${JOBS_DATABASE_URL}
      - AWS_LAMBDA_API_KEY=${AWS_LAMBDA_API_KEY}
      - LAMBDA_TRIGGER_WORKER_FUNCTION_URL=${LAMBDA_TRIGGER_WORKER_FUNCTION_URL}
      - API_SERVER_ROOT_URL=${API_SERVER_ROOT_URL}
      - SERVER_API_SECRET=${SERVER_API_SECRET}
      - ANALYTICS_SERVER_ROOT_URL=${ANALYTICS_ROOT_URL}
      - ANALYTICS_SERVER_SECRET_KEY=${ANALYTICS_SERVER_SECRET_KEY}
      - API_SERVER_SECRET_KEY=${SERVER_API_SECRET}
      - MAX_CONCURRENT_EVAL_JOBS=${MAX_CONCURRENT_EVAL_JOBS}
      - POOL_SIZE=5
      - ENV=local
      - NEW_RELIC_ENABLED=false

  analytics_db:
    profiles: ["core"]
    image: postgres:15.3-alpine
    restart: unless-stopped
    environment:
      - POSTGRES_PASSWORD=${ANALYTICS_DB_POSTGRES_PASSWORD}
      - POSTGRES_USER=${ANALYTICS_DB_POSTGRES_USER}
      - POSTGRES_DB=${ANALYTICS_DB_POSTGRES_DB_NAME}
    volumes:
      - analytics_db_postgres_data:/var/lib/postgresql/data
    container_name: analytics_db
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5

  analytics:
    profiles: ["core"]
    image: 867387325299.dkr.ecr.eu-central-1.amazonaws.com/athina-analytics:v1.8.5
    build:
      context: ../athina-analytics/.
      dockerfile: Dockerfile
    container_name: analytics
    ports:
      - "5000:5000"
    # Note: This depends_on is only needed if you are using the docker compose db.
    # If you are providing your own database url in the .env file, you can comment the depends_on part.
    links:
      - analytics_db
      - jobs_db
      - api
    depends_on:
      analytics_db:
        condition: service_healthy
      api:
        condition: service_started
    environment:
      - DATABASE_URL=${ANALYTICS_DATABASE_URL}
      - JOBS_DATABASE_URL=${JOBS_DATABASE_URL}
      - ROOT_URL=${ANALYTICS_ROOT_URL}
      - ANALYTICS_SERVER_API_SECRET=${ANALYTICS_SERVER_API_SECRET}
      - ENV=local
      - NODE_ENV=development
      - NEW_RELIC_ENABLED=false
      - NEW_RELIC_TRACER_ENABLED=${NEW_RELIC_TRACER_ENABLED}
      - NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=${NEW_RELIC_DISTRIBUTED_TRACING_ENABLED}
      - NEW_RELIC_SPAN_EVENTS_ENABLED=${NEW_RELIC_SPAN_EVENTS_ENABLED}
      - NEW_RELIC_DATASTORE_INSTANCE_REPORTING_ENABLED=${NEW_RELIC_DATASTORE_INSTANCE_REPORTING_ENABLED}
      - NEW_RELIC_APPLICATION_LOGGING_METRICS_ENABLED=${NEW_RELIC_APPLICATION_LOGGING_METRICS_ENABLED}
      - NEW_RELIC_CODE_LEVEL_METRICS_ENABLED=${NEW_RELIC_CODE_LEVEL_METRICS_ENABLED}
      - NEW_RELIC_APP_NAME=${NEW_RELIC_APP_NAME}
      - NEW_RELIC_LICENSE_KEY=${NEW_RELIC_LICENSE_KEY}
      - NEW_RELIC_LOG_LEVEL=${NEW_RELIC_LOG_LEVEL}

  gateway:
    profiles: ["core"]
    image: 867387325299.dkr.ecr.eu-central-1.amazonaws.com/athina-gateway:v1.8.22
    network_mode: "host"
    build:
      context: ../gateway/.
      dockerfile: Dockerfile
    container_name: gateway
    ports:
      - "7777:7777"
    environment:
      - API_SECRET=${GATEWAY_API_SECRET}
      - API_SERVER_ROOT_URL=${API_SERVER_CONTAINER_URL}
      - API_SERVER_SECRET_KEY=${SERVER_API_SECRET}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - WORKERS_SIZE=${WORKERS_SIZE}

  fastapi:
    profiles: ["worker"]
    image: 867387325299.dkr.ecr.eu-central-1.amazonaws.com/athina-workers:v1.8.13
    build:
      context: ../athina-workers/.
      dockerfile: ./Dockerfile-server
    ports:
      - "8000:8000"
    depends_on:
      - redis
    container_name: jobs_api
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
      - TOTAL_DATA_EXPORT_PROMPT_RUNS_SIZE=
      - API_SERVER_ROOT_URL=${API_SERVER_ROOT_URL}
      - DATABASE_URL=${JOBS_DATABASE_URL}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - API_SERVER_SECRET_KEY=${SERVER_API_SECRET}
      - ANALYTICS_SERVER_ROOT_URL=${ANALYTICS_ROOT_URL}
      - ANALYTICS_SERVER_SECRET_KEY=${ANALYTICS_SERVER_SECRET_KEY}
      - NUMBER_OF_RUNS=1
      - BATCH_SIZE=3
      - MAX_PROMPT_RUNS=10
      - SLEEP_BETWEEN_BATCHES_SECONDS=30
      - ATHINA_API_KEY=glH9JKd3Ox6gzDGrStXUG2OaCFjveMOQ
      - ENVIRONMENT=local
      - DATA_EXPORT_BUCKET_NAME=athina-export-files
      - DATA_EXPORT_PROMPT_RUNS_BATCH_SIZE=100
      - DATA_EXPORT_PROMPT_RUNS_OFFSET=0
      - ATHINA_ORG_ID=
      - DATA_IMPORT_BUCKET_NAME=athina-data-import
      - AWS_ACCESS_KEY_ID=test
      - AWS_SECRET_ACCESS_KEY=test
      - GATEWAY_URL=${GATEWAY_URL}
      - GATEWAY_SECRET=${GATEWAY_API_SECRET}
      - CLOUD_PROVIDER=server
      - GENERATE_DATASET_MAX_CONCURRENT_REQUESTS=3
      - WORKER_API_KEY=${WORKER_API_KEY}

  redis:
    profiles: ["worker"]
    container_name: jobs_queue
    image: "redis:7.0-alpine"
    restart: unless-stopped
    volumes:
      - redis-data:/data
    command: ["redis-server", "--appendonly", "yes"]

  worker:
    profiles: ["worker"]
    image: 867387325299.dkr.ecr.eu-central-1.amazonaws.com/athina-workers:v1.8.13
    build:
      context: ../athina-workers/.
      dockerfile: ./Dockerfile-server
    command: celery -A app.worker.celery_app worker --loglevel=info -Q process_job_queue
    container_name: jobs_worker
    depends_on:
      - redis
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
      - TOTAL_DATA_EXPORT_PROMPT_RUNS_SIZE=
      - API_SERVER_ROOT_URL=${API_SERVER_ROOT_URL}
      - DATABASE_URL=${JOBS_DATABASE_URL}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - API_SERVER_SECRET_KEY=${SERVER_API_SECRET}
      - ANALYTICS_SERVER_ROOT_URL=${ANALYTICS_ROOT_URL}
      - ANALYTICS_SERVER_SECRET_KEY=${ANALYTICS_SERVER_API_SECRET}
      - NUMBER_OF_RUNS=1
      - BATCH_SIZE=3
      - MAX_PROMPT_RUNS=10
      - SLEEP_BETWEEN_BATCHES_SECONDS=30
      - ATHINA_API_KEY=glH9JKd3Ox6gzDGrStXUG2OaCFjveMOQ
      - ENVIRONMENT=local
      - DATA_EXPORT_BUCKET_NAME=athina-export-files
      - DATA_EXPORT_PROMPT_RUNS_BATCH_SIZE=100
      - DATA_EXPORT_PROMPT_RUNS_OFFSET=0
      - ATHINA_ORG_ID=
      - DATA_IMPORT_BUCKET_NAME=athina-data-import
      - AWS_ACCESS_KEY_ID=test
      - AWS_SECRET_ACCESS_KEY=test
      - GATEWAY_URL=${GATEWAY_URL}
      - GATEWAY_SECRET=${GATEWAY_API_SECRET}
      - CLOUD_PROVIDER=server
      - GENERATE_DATASET_MAX_CONCURRENT_REQUESTS=3

  # Dozzle provides web UI for viewing and searching container logs
  dozzle:
    image: amir20/dozzle:v8.8.2
    container_name: dozzle
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./dozzle-users.yml:/data/users.yml
    ports:
      - "${DOZZLE_PORT}:8080"
    environment:
      DOZZLE_ADDR: ":${DOZZLE_PORT}"
      DOZZLE_BASE: '/'
      DOZZLE_AUTH_PROVIDER: simple
      DOZZLE_NO_ANALYTICS: true
    restart: unless-stopped

volumes:
  prompt_db_postgres_data:
  jobs_db_postgres_data:
  analytics_db_postgres_data:
  redis-data:
